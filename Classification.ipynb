{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- merge valid and train\n",
    "- generate new valid: https://www.kaggle.com/anqitu/data-distribution-eda-and-leaderboard-exploit/code\n",
    "- train linear classifier and compare to MLP performance (drop these features if there is no difference in performance)\n",
    "- try getting activations from earlier in the network\n",
    "- transfer learning: adjust later last layers of ResNet50 to our task\n",
    "- train CNN from scratch: http://cs231n.github.io/transfer-learning/\n",
    "- check scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', 'Classification.ipynb', 'data', 'feature_extractor.py', 'feature_extractor_batch.py', 'feature_extractor_linux.py', 'Initial exploration.ipynb', 'keras_test.py', 'LICENSE', 'mlp_200_100.pkl', 'out.csv', 'README.md', 'rescale.py', 'Silllina.ipynb', 'submission.csv', 'test.csv', 'test.json', 'TF-MLP.ipynb', 'train.json', 'try_this_one.csv', 'validation.json', 'warm_clfs.pkl', 'warm_clfs2.pkl', 'warm_clfs3.pkl', 'warm_clfs4.pkl']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "FEATURES_LOCATION = './data/features/'\n",
    "F_CORE = 'cnn_features_'\n",
    "\n",
    "def get_label_from_path(file):\n",
    "    return file.split('\\\\')[1].split('.')[0]\n",
    "\n",
    "def load_data(mode):\n",
    "    if(mode == 'test'):\n",
    "        pickle_path = F_CORE + mode\n",
    "        data = pickle.load(open(FEATURES_LOCATION + pickle_path + '.pkl', 'rb'))\n",
    "        to_return = {}\n",
    "        for key, value in list(data.items()):\n",
    "            to_return[get_label_from_path(key)] = value.reshape(1,-1)\n",
    "        return to_return, None\n",
    "    \n",
    "    pickle_path = F_CORE + mode + '_'\n",
    "    \n",
    "    data = {}\n",
    "    for i in range(1,129):\n",
    "        data[i] = pickle.load(open(FEATURES_LOCATION + pickle_path + str(i) + '.pkl', 'rb'))\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    for key, value in list(data.items()):\n",
    "        the_class = key\n",
    "        features = np.array(list(value.values()))\n",
    "        for feature in features:\n",
    "            y.append(the_class)\n",
    "            X.append(feature)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_data('train')\n",
    "X_val, y_val = load_data('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD3CAYAAAD/oDhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6fJ60hKLmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x235614aca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = np.ones_like(y) / (len(y))\n",
    "_ = plt.hist(y, weights=weights, bins=np.max(y), alpha=0.7)\n",
    "weights = np.ones_like(y_val) / (len(y_val))\n",
    "_ = plt.hist(y_val, weights=weights, bins=np.max(y_val), alpha=0.5)\n",
    "\n",
    "from collections import Counter\n",
    "counted = Counter(y)\n",
    "most_common_class = counted.most_common()[0][0]\n",
    "        \n",
    "print(counted[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X, X_val), axis=0)\n",
    "y = np.concatenate((y, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=200, alpha=0.25,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate='adaptive', learning_rate_init=.1)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(mlp, 'mlp_200_100.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.28873116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val score: 0.6292825473599355\n",
      "Iteration 2, loss = 1.22757467\n",
      "Val score: 0.6838976219266425\n",
      "Iteration 3, loss = 1.05957962\n",
      "Val score: 0.7043530834340992\n",
      "Iteration 4, loss = 0.97396514\n",
      "Val score: 0.7199214026602176\n",
      "Iteration 5, loss = 0.91685031\n",
      "Val score: 0.7296453043127771\n",
      "Iteration 6, loss = 0.87350552\n",
      "Val score: 0.736446997178557\n",
      "Iteration 7, loss = 0.83941594\n",
      "Val score: 0.7438029020556227\n",
      "Iteration 8, loss = 0.81027740\n",
      "Val score: 0.748690044336961\n",
      "Iteration 9, loss = 0.78619717\n",
      "Val score: 0.7507557436517533\n",
      "Iteration 10, loss = 0.76378661\n",
      "Val score: 0.7555925030229746\n",
      "Iteration 11, loss = 0.74458201\n",
      "Val score: 0.7582627972591697\n",
      "Iteration 12, loss = 0.72656728\n",
      "Val score: 0.7596231358323257\n",
      "Iteration 13, loss = 0.71045980\n",
      "Val score: 0.7627468762595727\n",
      "Iteration 14, loss = 0.69517952\n",
      "Val score: 0.7653667875856509\n",
      "Iteration 15, loss = 0.68135815\n",
      "Val score: 0.7672813381700927\n",
      "Iteration 16, loss = 0.66855619\n",
      "Val score: 0.7676844014510278\n",
      "Iteration 17, loss = 0.65600149\n",
      "Val score: 0.7700523982265216\n",
      "Iteration 18, loss = 0.64445148\n",
      "Val score: 0.7713623538895606\n",
      "Iteration 19, loss = 0.63340600\n",
      "Val score: 0.7744860943168077\n",
      "Iteration 20, loss = 0.62303851\n",
      "Val score: 0.7748891575977428\n",
      "Iteration 21, loss = 0.61350145\n",
      "Val score: 0.7795747682386135\n",
      "Iteration 22, loss = 0.60331430\n",
      "Val score: 0.7777106005642886\n",
      "Iteration 23, loss = 0.59428517\n",
      "Val score: 0.777509068923821\n",
      "Iteration 24, loss = 0.58574662\n",
      "Val score: 0.7785671100362757\n",
      "Iteration 25, loss = 0.57683138\n",
      "Val score: 0.7809351068117695\n",
      "Iteration 26, loss = 0.56897457\n",
      "Val score: 0.7791213220475615\n",
      "Iteration 27, loss = 0.56074485\n",
      "Val score: 0.7815397017331721\n",
      "Iteration 28, loss = 0.55328574\n",
      "Val score: 0.783403869407497\n",
      "Iteration 29, loss = 0.54594748\n",
      "Val score: 0.7836557839580814\n",
      "Iteration 30, loss = 0.53872079\n",
      "Val score: 0.7823962112051592\n",
      "Iteration 31, loss = 0.53166845\n",
      "Val score: 0.7841092301491334\n",
      "Iteration 32, loss = 0.52452923\n",
      "Val score: 0.7832527206771464\n",
      "Iteration 33, loss = 0.51777672\n",
      "Val score: 0.7857214832728738\n",
      "Iteration 34, loss = 0.51153453\n",
      "Val score: 0.7853184199919387\n",
      "Iteration 35, loss = 0.50493985\n",
      "Val score: 0.7847642079806529\n",
      "Iteration 36, loss = 0.49867027\n",
      "Val score: 0.7875352680370818\n",
      "Iteration 37, loss = 0.49295048\n",
      "Val score: 0.7892986698911729\n",
      "Iteration 38, loss = 0.48718251\n",
      "Val score: 0.785167271261588\n",
      "Iteration 39, loss = 0.48140561\n",
      "Val score: 0.7885429262394196\n",
      "Iteration 40, loss = 0.47563336\n",
      "Val score: 0.7873337363966143\n",
      "Iteration 41, loss = 0.47001948\n",
      "Val score: 0.7880390971382507\n",
      "Iteration 42, loss = 0.46444207\n",
      "Val score: 0.7901047964530431\n",
      "Iteration 43, loss = 0.45917893\n",
      "Val score: 0.7895002015316405\n",
      "Iteration 44, loss = 0.45410664\n",
      "Val score: 0.791062071745264\n",
      "Iteration 45, loss = 0.44876379\n",
      "Val score: 0.7912636033857315\n",
      "Iteration 46, loss = 0.44386533\n",
      "Val score: 0.7895505844417574\n",
      "Iteration 47, loss = 0.43900630\n",
      "Val score: 0.7886436920596533\n",
      "Iteration 48, loss = 0.43387275\n",
      "Val score: 0.7914651350261991\n",
      "Iteration 49, loss = 0.42913977\n",
      "Val score: 0.7914651350261991\n",
      "Iteration 50, loss = 0.42438486\n",
      "Val score: 0.7932789197904071\n",
      "Iteration 51, loss = 0.41957727\n",
      "Val score: 0.791968964127368\n",
      "Iteration 52, loss = 0.41518280\n",
      "Val score: 0.7904574768238614\n",
      "Iteration 53, loss = 0.41098787\n",
      "Val score: 0.7910116888351472\n",
      "Iteration 54, loss = 0.40629827\n",
      "Val score: 0.7911124546553809\n",
      "Iteration 55, loss = 0.40208424\n",
      "Val score: 0.7920193470374849\n",
      "Iteration 56, loss = 0.39741035\n",
      "Val score: 0.7937323659814591\n",
      "Iteration 57, loss = 0.39339667\n",
      "Val score: 0.7925735590487707\n",
      "Iteration 58, loss = 0.38907460\n",
      "Val score: 0.7930773881499396\n",
      "Iteration 59, loss = 0.38504370\n",
      "Val score: 0.7942865779927448\n",
      "Iteration 60, loss = 0.38116549\n",
      "Val score: 0.7938331318016929\n",
      "Iteration 61, loss = 0.37698272\n",
      "Val score: 0.7925735590487707\n",
      "Iteration 62, loss = 0.37300820\n",
      "Val score: 0.7934300685207578\n",
      "Iteration 63, loss = 0.36910771\n",
      "Val score: 0.7942865779927448\n",
      "Iteration 64, loss = 0.36516076\n",
      "Val score: 0.7936316001612254\n",
      "Iteration 65, loss = 0.36145887\n",
      "Val score: 0.7948911729141476\n",
      "Iteration 66, loss = 0.35745576\n",
      "Val score: 0.7937323659814591\n",
      "Iteration 67, loss = 0.35412409\n",
      "Val score: 0.794236195082628\n",
      "Iteration 68, loss = 0.35032285\n",
      "Val score: 0.7946392583635631\n",
      "Iteration 69, loss = 0.34634887\n",
      "Val score: 0.7948911729141476\n",
      "Iteration 70, loss = 0.34327545\n",
      "Val score: 0.7958988311164853\n",
      "Iteration 71, loss = 0.33945854\n",
      "Val score: 0.7931781539701733\n",
      "Iteration 72, loss = 0.33596161\n",
      "Val score: 0.7940346634421604\n",
      "Iteration 73, loss = 0.33285773\n",
      "Val score: 0.7940850463522773\n",
      "Iteration 74, loss = 0.32939246\n",
      "Val score: 0.7949919387343813\n",
      "Iteration 75, loss = 0.32588586\n",
      "Val score: 0.7947400241837969\n",
      "Iteration 76, loss = 0.32271167\n",
      "Val score: 0.7962011285771866\n",
      "Iteration 77, loss = 0.31921218\n",
      "Val score: 0.7944377267230955\n",
      "Iteration 78, loss = 0.31605133\n",
      "Val score: 0.794236195082628\n",
      "Iteration 79, loss = 0.31271231\n",
      "Val score: 0.7944377267230955\n",
      "Iteration 80, loss = 0.30986091\n",
      "Val score: 0.7952438532849657\n",
      "Iteration 81, loss = 0.30628434\n",
      "Val score: 0.7952438532849657\n",
      "Iteration 82, loss = 0.30347303\n",
      "Val score: 0.796906489318823\n",
      "Iteration 83, loss = 0.30024006\n",
      "Val score: 0.7936316001612254\n",
      "Iteration 84, loss = 0.29701311\n",
      "Val score: 0.7976622329705764\n",
      "Iteration 85, loss = 0.29429597\n",
      "Val score: 0.7954453849254333\n",
      "Iteration 86, loss = 0.29133922\n",
      "Val score: 0.7961003627569528\n",
      "Iteration 87, loss = 0.28843597\n",
      "Val score: 0.7961003627569528\n",
      "Iteration 88, loss = 0.28547494\n",
      "Val score: 0.7941354292623942\n",
      "Iteration 89, loss = 0.28259025\n",
      "Val score: 0.7948407900040306\n",
      "Iteration 90, loss = 0.27959659\n",
      "Val score: 0.7968057234985892\n",
      "Iteration 91, loss = 0.27680531\n",
      "Val score: 0.7945888754534461\n",
      "Iteration 92, loss = 0.27403408\n",
      "Val score: 0.7953446191051995\n",
      "Iteration 93, loss = 0.27157646\n",
      "Val score: 0.7959492140266021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val score: 0.7939842805320435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-7e279515f50f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mwarm_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarm_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mval_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    677\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             activations[i + 1] = safe_sparse_dot(activations[i],\n\u001b[1;32m--> 105\u001b[1;33m                                                  self.coefs_[i])\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.1)\n",
    "\n",
    "warm_mlp = MLPClassifier(hidden_layer_sizes=(1000,), max_iter=1, alpha=1e-4,\n",
    "            solver='sgd', verbose=10, tol=1e-4, random_state=None,\n",
    "            learning_rate='adaptive', learning_rate_init=.001,\n",
    "            warm_start=True)\n",
    "\n",
    "val_scores = []\n",
    "losses = []\n",
    "clfs = []\n",
    "for i in range(1,4000):\n",
    "    warm_mlp.fit(X_train,y_train)\n",
    "    val_score = warm_mlp.score(X_val, y_val)\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    losses.append(warm_mlp.loss_)\n",
    "    if(val_score > 0.78):\n",
    "        clfs.append(deepcopy(warm_mlp))\n",
    "    \n",
    "    print(\"Val score:\", val_score)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clfs, 'warm_clfs_512.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 118, loss = 0.16783498\n",
      "Val score: 0.7964530431277711\n",
      "Iteration 119, loss = 0.16611181\n",
      "Val score: 0.7971584038694075\n",
      "Iteration 120, loss = 0.16396161\n",
      "Val score: 0.798770656993148\n",
      "Iteration 121, loss = 0.16225765\n",
      "Val score: 0.7965538089480049\n",
      "Iteration 122, loss = 0.16040205\n",
      "Val score: 0.7970576380491737\n",
      "Iteration 123, loss = 0.15853175\n",
      "Val score: 0.7954453849254333\n",
      "Iteration 124, loss = 0.15677194\n",
      "Val score: 0.79695687222894\n",
      "Iteration 125, loss = 0.15485444\n",
      "Val score: 0.7975614671503426\n",
      "Iteration 126, loss = 0.15295885\n",
      "Val score: 0.7977629987908101\n",
      "Iteration 127, loss = 0.15128176\n",
      "Val score: 0.7977629987908101\n",
      "Iteration 128, loss = 0.14951657\n",
      "Val score: 0.7967553405884724\n",
      "Iteration 129, loss = 0.14800201\n",
      "Val score: 0.7959492140266021\n",
      "Iteration 130, loss = 0.14639875\n",
      "Val score: 0.7952438532849657\n",
      "Iteration 131, loss = 0.14457120\n",
      "Val score: 0.797359935509875\n",
      "Iteration 132, loss = 0.14293582\n",
      "Val score: 0.7970576380491737\n",
      "Iteration 133, loss = 0.14133046\n",
      "Val score: 0.7972591696896413\n",
      "Iteration 134, loss = 0.13976731\n",
      "Val score: 0.7965538089480049\n",
      "Iteration 135, loss = 0.13818892\n",
      "Val score: 0.7978637646110439\n",
      "Iteration 136, loss = 0.13652967\n",
      "Val score: 0.7970576380491737\n",
      "Iteration 137, loss = 0.13510154\n",
      "Val score: 0.7971584038694075\n",
      "Iteration 138, loss = 0.13352803\n",
      "Val score: 0.7966545747682386\n",
      "Iteration 139, loss = 0.13219140\n",
      "Val score: 0.79695687222894\n",
      "Iteration 140, loss = 0.13053618\n",
      "Val score: 0.7978637646110439\n",
      "Iteration 141, loss = 0.12912601\n",
      "Val score: 0.7976622329705764\n",
      "Iteration 142, loss = 0.12777203\n",
      "Val score: 0.7970576380491737\n",
      "Iteration 143, loss = 0.12623854\n",
      "Val score: 0.7966545747682386\n",
      "Iteration 144, loss = 0.12501525\n",
      "Val score: 0.795143087464732\n",
      "Iteration 145, loss = 0.12355035\n",
      "Val score: 0.7981660620717452\n",
      "Iteration 146, loss = 0.12219429\n",
      "Val score: 0.7989721886336155\n",
      "Iteration 147, loss = 0.12080429\n",
      "Val score: 0.7970576380491737\n",
      "Iteration 148, loss = 0.11956892\n",
      "Val score: 0.7985691253526803\n",
      "Iteration 149, loss = 0.11799891\n",
      "Val score: 0.7980652962515115\n",
      "Iteration 150, loss = 0.11696302\n",
      "Val score: 0.797359935509875\n",
      "Iteration 151, loss = 0.11565591\n",
      "Val score: 0.7954453849254333\n",
      "Iteration 152, loss = 0.11449345\n",
      "Val score: 0.7967553405884724\n",
      "Iteration 153, loss = 0.11324306\n",
      "Val score: 0.7952438532849657\n",
      "Iteration 154, loss = 0.11198302\n",
      "Val score: 0.7964530431277711\n",
      "Iteration 155, loss = 0.11067916\n",
      "Val score: 0.7983675937122128\n",
      "Iteration 156, loss = 0.10966985\n",
      "Val score: 0.7994760177347844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['warm_clfs4.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,40):\n",
    "    warm_mlp.fit(X_train,y_train)\n",
    "    val_score = warm_mlp.score(X_val, y_val)\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    losses.append(warm_mlp.loss_)\n",
    "    if(val_score > 0.785):\n",
    "        clfs.append(deepcopy(warm_mlp))\n",
    "    \n",
    "    print(\"Val score:\", val_score)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clfs, 'warm_clfs4.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD3CAYAAAAALt/WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRS6EEH7u/5mS85NJ/kZeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x167e31032b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), 1 - np.array(val_scores), 'g')\n",
    "plt.plot(range(len(losses)), losses / max(losses), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, _ = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99990\r"
     ]
    }
   ],
   "source": [
    "# ONE PREDICTION AT A TIME\n",
    "predictions = {}\n",
    "for index, feature_vector in X_test.items():\n",
    "    predictions[int(index)] = the_clf.predict(feature_vector)[0]\n",
    "    print(index, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BATCH OF PREDICTIONS\n",
    "X_test_arr = np.array(list(X_test.values()))\n",
    "X_test_arr = X_test_arr.reshape(-1,2048)\n",
    "\n",
    "y_pred = the_clf.predict(X_test_arr)\n",
    "predictions2 = {}\n",
    "for i, index in enumerate(X_test.keys()):\n",
    "    predictions2[int(index)] = y_pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_pred = deepcopy(predictions)\n",
    "predictions = predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counted = Counter(predictions.values())\n",
    "most_common_class = counted.most_common()[0][0]\n",
    "\n",
    "for index in range(1, 12801):\n",
    "    if(index not in predictions.keys()):\n",
    "        predictions[index] = most_common_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "values = []\n",
    "for key, value in predictions.items():\n",
    "    ids.append(key)\n",
    "    values.append(value)\n",
    "    \n",
    "out_dict = {}\n",
    "out_dict['id'] = ids\n",
    "out_dict['predicted'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = sorted(out_dict.keys())\n",
    "COL_WIDTH = 6\n",
    "FMT = \"%%-%ds\" % COL_WIDTH\n",
    "\n",
    "with open('try_this_one.csv', 'w') as csv:\n",
    "    # Write keys    \n",
    "    csv.write(','.join([k for k in keys]) + '\\n')\n",
    "\n",
    "    # Assume all values of dict are equal\n",
    "    for i in range(len(out_dict[keys[0]])):\n",
    "        csv.write(','.join([FMT % out_dict[k][i] for k in keys]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.89798529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val score: 0.5282144296654575\n",
      "Iteration 2, loss = 1.58595645\n",
      "Val score: 0.609230149133414\n",
      "Iteration 3, loss = 1.33435703\n",
      "Val score: 0.6424324869004434\n",
      "Iteration 4, loss = 1.21493818\n",
      "Val score: 0.6618802902055623\n",
      "Iteration 5, loss = 1.14020006\n",
      "Val score: 0.673720274083031\n",
      "Iteration 6, loss = 1.08726612\n",
      "Val score: 0.6847541313986296\n",
      "Iteration 7, loss = 1.04640361\n",
      "Val score: 0.6925634824667473\n",
      "Iteration 8, loss = 1.01393233\n",
      "Val score: 0.6994155582426441\n",
      "Iteration 9, loss = 0.98667286\n",
      "Val score: 0.70304312777106\n",
      "Iteration 10, loss = 0.96353372\n",
      "Val score: 0.7085348649738009\n",
      "Iteration 11, loss = 0.94332357\n",
      "Val score: 0.7119609028617493\n",
      "Iteration 12, loss = 0.92558716\n",
      "Val score: 0.7159411527609835\n",
      "Iteration 13, loss = 0.90971104\n",
      "Val score: 0.7189137444578799\n",
      "Iteration 14, loss = 0.89523302\n",
      "Val score: 0.7205259975816203\n",
      "Iteration 15, loss = 0.88221576\n",
      "Val score: 0.7228939943571141\n",
      "Iteration 16, loss = 0.87020693\n",
      "Val score: 0.7255642885933091\n",
      "Iteration 17, loss = 0.85904227\n",
      "Val score: 0.7270757758968158\n",
      "Iteration 18, loss = 0.84864316\n",
      "Val score: 0.7289399435711407\n",
      "Iteration 19, loss = 0.83909227\n",
      "Val score: 0.7311567916162838\n",
      "Iteration 20, loss = 0.83008575\n",
      "Val score: 0.7317110036275696\n",
      "Iteration 21, loss = 0.82155590\n",
      "Val score: 0.7328698105602579\n",
      "Iteration 22, loss = 0.81341474\n",
      "Val score: 0.7348851269649335\n",
      "Iteration 23, loss = 0.80594291\n",
      "Val score: 0.7366989117291415\n",
      "Iteration 24, loss = 0.79877709\n",
      "Val score: 0.738260781942765\n",
      "Iteration 25, loss = 0.79191502\n",
      "Val score: 0.7383615477629988\n",
      "Iteration 26, loss = 0.78527586\n",
      "Val score: 0.7398226521563885\n",
      "Iteration 27, loss = 0.77912588\n",
      "Val score: 0.7410822249093107\n",
      "Iteration 28, loss = 0.77297220\n",
      "Val score: 0.7428960096735188\n",
      "Iteration 29, loss = 0.76732342\n",
      "Val score: 0.742341797662233\n",
      "Iteration 30, loss = 0.76179849\n",
      "Val score: 0.7426440951229343\n",
      "Iteration 31, loss = 0.75635109\n",
      "Val score: 0.7450624748085449\n",
      "Iteration 32, loss = 0.75142962\n",
      "Val score: 0.7444578798871423\n",
      "Iteration 33, loss = 0.74643090\n",
      "Val score: 0.7453143893591294\n",
      "Iteration 34, loss = 0.74145331\n",
      "Val score: 0.7455663039097138\n",
      "Iteration 35, loss = 0.73704248\n",
      "Val score: 0.7471281741233373\n",
      "Iteration 36, loss = 0.73249481\n",
      "Val score: 0.746825876662636\n",
      "Iteration 37, loss = 0.72816187\n",
      "Val score: 0.7472289399435711\n",
      "Iteration 38, loss = 0.72403544\n",
      "Val score: 0.7481358323256752\n",
      "Iteration 39, loss = 0.71981847\n",
      "Val score: 0.7483877468762595\n",
      "Iteration 40, loss = 0.71593854\n",
      "Val score: 0.7491434905280129\n",
      "Iteration 41, loss = 0.71201712\n",
      "Val score: 0.7500503829101169\n",
      "Iteration 42, loss = 0.70826914\n",
      "Val score: 0.7495969367190649\n",
      "Iteration 43, loss = 0.70457426\n",
      "Val score: 0.7498488512696493\n",
      "Iteration 44, loss = 0.70117557\n",
      "Val score: 0.7505038291011689\n",
      "Iteration 45, loss = 0.69752426\n",
      "Val score: 0.7516626360338573\n",
      "Iteration 46, loss = 0.69413286\n",
      "Val score: 0.751813784764208\n",
      "Iteration 47, loss = 0.69062390\n",
      "Val score: 0.7510580411124547\n",
      "Iteration 48, loss = 0.68739851\n",
      "Val score: 0.7524687625957275\n",
      "Iteration 49, loss = 0.68425968\n",
      "Val score: 0.7508061265618702\n",
      "Iteration 50, loss = 0.68106897\n",
      "Val score: 0.7537787182587666\n",
      "Iteration 51, loss = 0.67805590\n",
      "Val score: 0.751813784764208\n",
      "Iteration 52, loss = 0.67519436\n",
      "Val score: 0.7528214429665457\n",
      "Iteration 53, loss = 0.67200639\n",
      "Val score: 0.7522672309552599\n",
      "Iteration 54, loss = 0.66935368\n",
      "Val score: 0.7522672309552599\n",
      "Iteration 55, loss = 0.66628934\n",
      "Val score: 0.7549375251914551\n",
      "Iteration 56, loss = 0.66364823\n",
      "Val score: 0.7530733575171302\n",
      "Iteration 57, loss = 0.66099671\n",
      "Val score: 0.7537787182587666\n",
      "Iteration 58, loss = 0.65838605\n",
      "Val score: 0.7537787182587666\n",
      "Iteration 59, loss = 0.65582840\n",
      "Val score: 0.7559451833937928\n",
      "Iteration 60, loss = 0.65316529\n",
      "Val score: 0.7546856106408706\n",
      "Iteration 61, loss = 0.65060565\n",
      "Val score: 0.7553909713825071\n",
      "Iteration 62, loss = 0.64815820\n",
      "Val score: 0.7555925030229746\n",
      "Iteration 63, loss = 0.64586296\n",
      "Val score: 0.7560963321241435\n",
      "Iteration 64, loss = 0.64327396\n",
      "Val score: 0.7559955663039097\n",
      "Iteration 65, loss = 0.64092680\n",
      "Val score: 0.7549879081015719\n",
      "Iteration 66, loss = 0.63852576\n",
      "Val score: 0.7561467150342603\n",
      "Iteration 67, loss = 0.63637941\n",
      "Val score: 0.7552398226521564\n",
      "Iteration 68, loss = 0.63400142\n",
      "Val score: 0.7573559048770657\n",
      "Iteration 69, loss = 0.63185052\n",
      "Val score: 0.7555421201128577\n",
      "Iteration 70, loss = 0.62971949\n",
      "Val score: 0.7574566706972995\n",
      "Iteration 71, loss = 0.62764208\n",
      "Val score: 0.7554413542926239\n",
      "Iteration 72, loss = 0.62546335\n",
      "Val score: 0.757204756146715\n",
      "Iteration 73, loss = 0.62342784\n",
      "Val score: 0.7586658605401048\n",
      "Iteration 74, loss = 0.62141648\n",
      "Val score: 0.7566001612253124\n",
      "Iteration 75, loss = 0.61936345\n",
      "Val score: 0.7564993954050786\n",
      "Iteration 76, loss = 0.61743569\n",
      "Val score: 0.7575574365175333\n",
      "Iteration 77, loss = 0.61538415\n",
      "Val score: 0.7585147118097542\n",
      "Iteration 78, loss = 0.61342495\n",
      "Val score: 0.7562474808544941\n",
      "Iteration 79, loss = 0.61154140\n",
      "Val score: 0.7584643288996372\n",
      "Iteration 80, loss = 0.60975989\n",
      "Val score: 0.7562978637646111\n",
      "Iteration 81, loss = 0.60782739\n",
      "Val score: 0.7582124143490528\n",
      "Iteration 82, loss = 0.60581230\n",
      "Val score: 0.757204756146715\n",
      "Iteration 83, loss = 0.60418808\n",
      "Val score: 0.7571039903264812\n",
      "Iteration 84, loss = 0.60228719\n",
      "Val score: 0.7569024586860137\n",
      "Iteration 85, loss = 0.60059674\n",
      "Val score: 0.7577589681580008\n",
      "Iteration 86, loss = 0.59892924\n",
      "Val score: 0.7579101168883515\n",
      "Iteration 87, loss = 0.59719000\n",
      "Val score: 0.7571543732365982\n",
      "Iteration 88, loss = 0.59540272\n",
      "Val score: 0.7581620314389359\n",
      "Iteration 89, loss = 0.59380194\n",
      "Val score: 0.757658202337767\n",
      "Iteration 90, loss = 0.59208549\n",
      "Val score: 0.757658202337767\n",
      "Iteration 91, loss = 0.59043176\n",
      "Val score: 0.7585147118097542\n",
      "Iteration 92, loss = 0.58877778\n",
      "Val score: 0.7581620314389359\n",
      "Iteration 93, loss = 0.58722718\n",
      "Val score: 0.7577589681580008\n",
      "Iteration 94, loss = 0.58557219\n",
      "Val score: 0.7581620314389359\n",
      "Iteration 95, loss = 0.58405321\n",
      "Val score: 0.7579604997984684\n",
      "Iteration 96, loss = 0.58249085\n",
      "Val score: 0.7580612656187021\n",
      "Iteration 97, loss = 0.58099412\n",
      "Val score: 0.758565094719871\n",
      "Iteration 98, loss = 0.57951819\n",
      "Val score: 0.7589681580008061\n",
      "Iteration 99, loss = 0.57788618\n",
      "Val score: 0.7581620314389359\n",
      "Iteration 100, loss = 0.57639768\n",
      "Val score: 0.7581116485288191\n",
      "Iteration 101, loss = 0.57494466\n",
      "Val score: 0.7578597339782346\n",
      "Iteration 102, loss = 0.57353261\n",
      "Val score: 0.758565094719871\n",
      "Iteration 103, loss = 0.57204217\n",
      "Val score: 0.7589177750906893\n",
      "Iteration 104, loss = 0.57051109\n",
      "Val score: 0.7581116485288191\n",
      "Iteration 105, loss = 0.56918629\n",
      "Val score: 0.7587162434502217\n",
      "Iteration 106, loss = 0.56797513\n",
      "Val score: 0.7590689238210399\n",
      "Iteration 107, loss = 0.56656094\n",
      "Val score: 0.7587162434502217\n",
      "Iteration 108, loss = 0.56505151\n",
      "Val score: 0.758565094719871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val score: 0.7578597339782346\n",
      "Iteration 109, loss = 0.56385029\n",
      "Val score: 0.759471987101975\n",
      "Iteration 110, loss = 0.56248975\n",
      "Val score: 0.7578597339782346\n",
      "Iteration 111, loss = 0.56100910\n",
      "Val score: 0.7582627972591697\n",
      "Iteration 112, loss = 0.55975389\n",
      "Val score: 0.7593208383716243\n",
      "Iteration 113, loss = 0.55852013\n",
      "Val score: 0.7589177750906893\n",
      "Iteration 114, loss = 0.55703085\n",
      "Val score: 0.758565094719871\n",
      "Iteration 115, loss = 0.55585072\n",
      "Val score: 0.7582627972591697\n",
      "Iteration 116, loss = 0.55466461\n",
      "Val score: 0.7582627972591697\n",
      "Iteration 117, loss = 0.55324098\n",
      "Val score: 0.759018540910923\n",
      "Iteration 118, loss = 0.55189542\n",
      "Val score: 0.7581620314389359\n",
      "Iteration 119, loss = 0.55081010\n",
      "Val score: 0.7583131801692866\n",
      "Iteration 120, loss = 0.54962518\n",
      "Val score: 0.7578597339782346\n",
      "Iteration 121, loss = 0.54837312\n",
      "Val score: 0.7592200725513906\n",
      "Iteration 122, loss = 0.54722793\n",
      "Val score: 0.759018540910923\n",
      "Iteration 123, loss = 0.54599857\n",
      "Val score: 0.7582627972591697\n",
      "Iteration 124, loss = 0.54485038\n",
      "Val score: 0.7595223700120919\n",
      "Val score: 0.7585147118097542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2f7f9736f81f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwarm_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarm_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mval_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    676\u001b[0m                                          layer_units[i + 1])))\n\u001b[0;32m    677\u001b[0m         \u001b[1;31m# forward propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# For the last layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0moutput_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mACTIVATIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_activation_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Michal\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_base.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \"\"\"\n\u001b[0;32m     91\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.1)\n",
    "warm_mlp = MLPClassifier(hidden_layer_sizes=(200,), max_iter=1, alpha=1e-4,\n",
    "            solver='sgd', verbose=10, tol=1e-4, random_state=None,\n",
    "            learning_rate='adaptive', learning_rate_init=.001,\n",
    "            warm_start=True)\n",
    "\n",
    "val_scores = []\n",
    "losses = []\n",
    "clfs = []\n",
    "for i in range(1,4000):\n",
    "    warm_mlp.fit(X_train,y_train)\n",
    "    val_score = warm_mlp.score(X_val, y_val)\n",
    "    \n",
    "    val_scores.append(val_score)\n",
    "    losses.append(warm_mlp.loss_)\n",
    "    if(val_score > 0.7):\n",
    "        clfs.append(deepcopy(warm_mlp))\n",
    "    \n",
    "    print(\"Val score:\", val_score)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(clfs, 'clfs_250features.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'n_iter_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-25306f0d5028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'n_iter_'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         'data/train',\n",
    "#         target_size=(150, 150),\n",
    "#         batch_size=32,\n",
    "#         class_mode='binary')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#         'data/validation',\n",
    "#         target_size=(150, 150),\n",
    "#         batch_size=32,\n",
    "#         class_mode='binary')\n",
    "\n",
    "# train_generator.flow(x, y=None, batch_size=32, shuffle=True, seed=None, save_to_dir='./gen_test', save_prefix='', save_format='png', subset=None)\n",
    "iterator = train_datagen.flow_from_directory('data/valid', target_size=(244, 244), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, \n",
    "                                    seed=None, save_to_dir='gen_test', save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest')\n",
    "\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
