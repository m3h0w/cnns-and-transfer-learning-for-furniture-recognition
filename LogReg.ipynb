{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "FEATURES_LOCATION = './data/features/'\n",
    "F_CORE = 'cnn_features_'\n",
    "\n",
    "def get_label_from_path(file):\n",
    "    return file.split('\\\\')[1].split('.')[0]\n",
    "\n",
    "def load_data(mode):\n",
    "    if(mode == 'test'):\n",
    "        pickle_path = F_CORE + mode\n",
    "        data = pickle.load(open(FEATURES_LOCATION + pickle_path + '.pkl', 'rb'))\n",
    "        to_return = {}\n",
    "        for key, value in list(data.items()):\n",
    "            to_return[get_label_from_path(key)] = value.reshape(1,-1)\n",
    "        return to_return, None\n",
    "    \n",
    "    pickle_path = F_CORE + mode + '_'\n",
    "    \n",
    "    data = {}\n",
    "    for i in range(1,129):\n",
    "        data[i] = pickle.load(open(FEATURES_LOCATION + pickle_path + str(i) + '.pkl', 'rb'))\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    for key, value in list(data.items()):\n",
    "        the_class = key\n",
    "        features = np.array(list(value.values()))\n",
    "        for feature in features:\n",
    "            y.append(the_class)\n",
    "            X.append(feature)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = load_data('train')\n",
    "X_val, y_val = load_data('valid')\n",
    "\n",
    "# Extract number of labels in the training data\n",
    "num_labels = np.unique(y).shape[0]\n",
    "num_features = X.shape[1]\n",
    "num_trainobs = X.shape[0]\n",
    "\n",
    "# Create one hot encoding for training and validation features\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y)\n",
    "y = lb.transform(y)\n",
    "y_val = lb.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test, _ = load_data('test')\n",
    "len(X_test.items())\n",
    "\n",
    "X_test_arr = np.array(list(X_test.values()))\n",
    "X_test_arr = X_test_arr.reshape(-1,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-379710ece7ac>:25: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow graph set up\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Variables\n",
    "    batch_size = 10000 # mini batch for SGD\n",
    "    lamb = 0.002 # regularization (0.001 - 0.01 seems good)\n",
    "    learn_rate = 0.25 # learning rate (0.2 - 0.3 seems good with regularization)\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(X_val)\n",
    "    tf_test_dataset = tf.constant(X_test_arr)\n",
    "    \n",
    "    # Initial weights and biases for output/logit layer\n",
    "    w_logit = tf.Variable(tf.random_normal([num_features, num_labels]))\n",
    "    b_logit = tf.Variable(tf.random_normal([num_labels]))\n",
    "    \n",
    "    def model(data):\n",
    "        return tf.matmul(data, w_logit) + b_logit\n",
    "    \n",
    "    # Training computations\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    regularized_loss = tf.nn.l2_loss(w_logit)\n",
    "    total_loss = loss + lamb * regularized_loss\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(total_loss)\n",
    "    \n",
    "    # Predictions for training, validation and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return(100 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 108.045074\n",
      "Minibatch accuracy: 0.5%\n",
      "Validation accuracy: 0.8%\n",
      "Minibatch loss at step 1000: 6.493217\n",
      "Minibatch accuracy: 56.6%\n",
      "Validation accuracy: 50.7%\n",
      "Minibatch loss at step 2000: 2.755122\n",
      "Minibatch accuracy: 64.4%\n",
      "Validation accuracy: 58.9%\n",
      "Minibatch loss at step 3000: 1.300885\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 4000: 0.811842\n",
      "Minibatch accuracy: 77.4%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 5000: 0.658147\n",
      "Minibatch accuracy: 81.3%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 6000: 0.618627\n",
      "Minibatch accuracy: 82.9%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 7000: 0.619211\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 8000: 0.619958\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 9000: 0.630819\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 10000: 0.616471\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 11000: 0.629642\n",
      "Minibatch accuracy: 82.9%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 12000: 0.613028\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 13000: 0.617078\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 14000: 0.622415\n",
      "Minibatch accuracy: 83.7%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 15000: 0.606458\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 75.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Generate minibatch\n",
    "        ind = np.random.choice(num_trainobs, size = batch_size, replace = False)\n",
    "        batch_data = X[ind, :]\n",
    "        batch_labels = y[ind, :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 1000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), y_val))\n",
    "            #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), y_test))\n",
    "    \n",
    "    predictionstf = test_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions from one-hot to actual labels and print csv\n",
    "y_pred = lb.inverse_transform(predictionstf)\n",
    "\n",
    "predictions = {}\n",
    "for i, index in enumerate(X_test.keys()):\n",
    "    predictions[int(index)] = y_pred[i]\n",
    "    \n",
    "from collections import Counter\n",
    "counted = Counter(predictions.values())\n",
    "most_common_class = counted.most_common()[0][0]\n",
    "\n",
    "for index in range(1, 12801):\n",
    "    if(index not in predictions.keys()):\n",
    "        predictions[index] = most_common_class\n",
    "        \n",
    "ids = []\n",
    "values = []\n",
    "for key, value in predictions.items():\n",
    "    ids.append(key)\n",
    "    values.append(value)\n",
    "    \n",
    "out_dict = {}\n",
    "out_dict['id'] = ids\n",
    "out_dict['predicted'] = values\n",
    "\n",
    "keys = sorted(out_dict.keys())\n",
    "COL_WIDTH = 6\n",
    "FMT = \"%%-%ds\" % COL_WIDTH\n",
    "\n",
    "with open('predictions_v2.csv', 'w') as csv:\n",
    "    # Write keys    \n",
    "    csv.write(','.join([k for k in keys]) + '\\n')\n",
    "\n",
    "    # Assume all values of dict are equal\n",
    "    for i in range(len(out_dict[keys[0]])):\n",
    "        csv.write(','.join([FMT % out_dict[k][i] for k in keys]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
