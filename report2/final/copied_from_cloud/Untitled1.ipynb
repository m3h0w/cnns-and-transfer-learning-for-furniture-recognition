{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1nnuser/lsdaenv/lib/python3.5/site-packages/skimage/io/_plugins/matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n",
      "/home/1nnuser/lsdaenv/lib/python3.5/site-packages/matplotlib/axes/_base.py:1400: MatplotlibDeprecationWarning: The 'box-forced' keyword argument is deprecated since 2.2.\n",
      "  \" since 2.2.\", cbook.mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage import data, io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def imread(file):\n",
    "    try:\n",
    "        img = io.imread(file+'.jpg')\n",
    "    except:\n",
    "        try:\n",
    "            img = io.imread(file+'.jpeg')\n",
    "        except:\n",
    "            raise\n",
    "    return img\n",
    "\n",
    "img = imread('/furniture-data/test/1')\n",
    "io.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/1nnuser/lsdaenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import glob, os, sys\n",
    "import pickle\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# model = load_model('model.h5')\n",
    "model = ResNet50(weights='imagenet', include_top=False)\n",
    "# get_features_layer_output = K.function([model.layers[0].input],\n",
    "#                                   [model.layers[-3].output])\n",
    "# n_samples = 12000\n",
    "# progress_bar = tqdm(total=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/furniture-data/test/1.jpg\n"
     ]
    }
   ],
   "source": [
    "file = '/furniture-data/test/1.jpg'\n",
    "print(file)\n",
    "img = image.load_img(file, target_size=(224, 224))\n",
    "# print(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# layer_output = get_features_layer_output([x])[0]\n",
    "#     progress_bar.update(multiply)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(file, multiply):\n",
    "    print(file)\n",
    "    img = image.load_img(file, target_size=(224, 224))\n",
    "    # print(img)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    # layer_output = get_features_layer_output([x])[0]\n",
    "#     progress_bar.update(multiply)\n",
    "    return [file, model.predict(x).reshape(-1)]\n",
    "\n",
    "def extract_features(mode):\n",
    "    data_path = '/furniture-data/'\n",
    "    pickle_name_core = data_path + 'features/cnn_features_'+mode\n",
    "    \n",
    "\n",
    "    if mode == 'test':\n",
    "        pickle_name = pickle_name_core + '.pkl'\n",
    "        try:\n",
    "            dict_prev = pickle.load(open(pickle_name, 'rb'))\n",
    "        except:\n",
    "            dict_prev = {}\n",
    "        dict_len = len(dict_prev.keys())\n",
    "        \n",
    "        if(dict_prev):\n",
    "            print(pickle_name, \" already exists.\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Extracting features for: \", mode)\n",
    "\n",
    "        files = glob.glob(data_path + mode + \"/*\")\n",
    "        print(files)\n",
    "#         progress_bar.total = len(files)\n",
    "        n_jobs = 4\n",
    "        print(\"FILES:\", len(files))\n",
    "        X = Parallel(n_jobs=n_jobs)(delayed(get_features)(file, n_jobs) for file in files)\n",
    "        print(\"len of X\", len(X))\n",
    "        \n",
    "        if(len(X) == 0):\n",
    "            raise\n",
    "        # if(len(X) != n_samples)\n",
    "        #     n_iters += 1\n",
    "        # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        dict_new = {item[0]: item[1] for item in X}\n",
    "\n",
    "        with open(pickle_name, 'wb') as f:\n",
    "            pickle.dump(dict_new, f)\n",
    "        return\n",
    "        \n",
    "    for batch_i in range(1,129):\n",
    "        \n",
    "        pickle_name = pickle_name_core + '_' + str(batch_i) + '.pkl'\n",
    "        try:\n",
    "            dict_prev = pickle.load(open(pickle_name, 'rb'))\n",
    "        except:\n",
    "            dict_prev = {}\n",
    "        dict_len = len(dict_prev.keys())\n",
    "        \n",
    "        if(dict_prev):\n",
    "            print(pickle_name, \" already exists.\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Extracting features for: \", mode, batch_i)\n",
    "        \n",
    "        X = list()\n",
    "        start_time = time.time()\n",
    "        files = glob.glob(data_path + mode + \"/\" + str(batch_i) + \"/*\")\n",
    "#         progress_bar.total = len(files)\n",
    "        [X.append(get_features(file, 1)) for file in files]\n",
    "#         n_jobs = 4\n",
    "        print(\"FILES:\", len(files))\n",
    "#         X = Parallel(n_jobs=n_jobs)(delayed(get_features)(file, n_jobs) for file in files)\n",
    "        print(\"len of X\", len(X))\n",
    "        \n",
    "        if(len(X) == 0):\n",
    "            raise\n",
    "        # if(len(X) != n_samples)\n",
    "        #     n_iters += 1\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        dict_new = {item[0]: item[1] for item in X}\n",
    "\n",
    "        with open(pickle_name, 'wb') as f:\n",
    "            pickle.dump(dict_new, f)\n",
    "        \n",
    "    # X = pickle.load(open('hist_features.pkl', 'rb'))\n",
    "    # print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_features('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
